{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9384777,"sourceType":"datasetVersion","datasetId":5693856},{"sourceId":9399301,"sourceType":"datasetVersion","datasetId":5705244},{"sourceId":9403424,"sourceType":"datasetVersion","datasetId":5708645}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Directory","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-14T08:54:49.654453Z","iopub.execute_input":"2024-09-14T08:54:49.655505Z","iopub.status.idle":"2024-09-14T08:54:49.673217Z","shell.execute_reply.started":"2024-09-14T08:54:49.655443Z","shell.execute_reply":"2024-09-14T08:54:49.672077Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/amazon-ml/sample_test.csv\n/kaggle/input/amazon-ml/sample_test_out_fail.csv\n/kaggle/input/amazon-ml/sample_test_out.csv\n/kaggle/input/amazon-ml/train.csv\n/kaggle/input/amazon-ml/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Imports & Others","metadata":{}},{"cell_type":"code","source":"!pip install efficientnet_pytorch","metadata":{"execution":{"iopub.status.busy":"2024-09-14T08:54:50.125208Z","iopub.execute_input":"2024-09-14T08:54:50.125747Z","iopub.status.idle":"2024-09-14T08:55:10.709410Z","shell.execute_reply.started":"2024-09-14T08:54:50.125696Z","shell.execute_reply":"2024-09-14T08:55:10.707796Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet_pytorch) (2.4.0+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.5)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\nBuilding wheels for collected packages: efficientnet_pytorch\n  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16427 sha256=a8a4cfe569c7926cecf0db4346d18474c08c95be5ab4362164070ecab86c9fbd\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\nSuccessfully built efficientnet_pytorch\nInstalling collected packages: efficientnet_pytorch\nSuccessfully installed efficientnet_pytorch-0.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport random\nimport warnings\nimport requests\n# import multiprocessing\nimport re\nimport time\nimport urllib\nfrom time import time as timer\nfrom pathlib import Path\nfrom io import BytesIO\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport easyocr\n# import pytesseract\nimport cv2\n# from efficientnet_pytorch import EfficientNet\nfrom functools import partial\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast\nfrom torchvision import transforms\n\nfrom transformers import BertTokenizer, BertModel\n\n\nprint(\"Libraries Imported!\")","metadata":{"execution":{"iopub.status.busy":"2024-09-14T08:55:19.952860Z","iopub.execute_input":"2024-09-14T08:55:19.953339Z","iopub.status.idle":"2024-09-14T08:55:28.902401Z","shell.execute_reply.started":"2024-09-14T08:55:19.953277Z","shell.execute_reply":"2024-09-14T08:55:28.901007Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Libraries Imported!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Suppress specific warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-09-14T08:55:28.905038Z","iopub.execute_input":"2024-09-14T08:55:28.905885Z","iopub.status.idle":"2024-09-14T08:55:28.911688Z","shell.execute_reply.started":"2024-09-14T08:55:28.905808Z","shell.execute_reply":"2024-09-14T08:55:28.910466Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Check if CUDA (GPU) is available\nif torch.cuda.is_available():\n    # Get the number of available GPUs\n    num_gpus = torch.cuda.device_count()\n    \n    # Loop through each GPU and print its name\n    for i in range(num_gpus):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\nelse:\n    print(\"No GPU available\")","metadata":{"execution":{"iopub.status.busy":"2024-09-14T08:55:28.913507Z","iopub.execute_input":"2024-09-14T08:55:28.914008Z","iopub.status.idle":"2024-09-14T08:55:28.928832Z","shell.execute_reply.started":"2024-09-14T08:55:28.913965Z","shell.execute_reply":"2024-09-14T08:55:28.927553Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"No GPU available\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Check for GPU availability\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {device}\")\n\n# Hyperparameters\nBATCH_SIZE = 32\nEPOCHS = 5\nLEARNING_RATE = 1e-4\nMAX_LEN = 128\nNUM_WORKERS = 0\n\n# Initialize EasyOCR Reader for text extraction\n# Step 1: Set GPU 1 for EasyOCR\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\n# Step 2: Initialize EasyOCR, this will use GPU 1\n# reader = easyocr.Reader(['en'], gpu=True)\nreader = easyocr.Reader(['en'], gpu=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T08:55:31.551290Z","iopub.execute_input":"2024-09-14T08:55:31.551839Z","iopub.status.idle":"2024-09-14T08:55:38.715921Z","shell.execute_reply.started":"2024-09-14T08:55:31.551790Z","shell.execute_reply":"2024-09-14T08:55:38.714820Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"entity_unit_map = {\n    \"width\": {\"centimetre\", \"foot\", \"millimetre\", \"metre\", \"inch\", \"yard\"},\n    \"depth\": {\"centimetre\", \"foot\", \"millimetre\", \"metre\", \"inch\", \"yard\"},\n    \"height\": {\"centimetre\", \"foot\", \"millimetre\", \"metre\", \"inch\", \"yard\"},\n    \"item_weight\": {\"milligram\", \"kilogram\", \"microgram\", \"gram\", \"ounce\", \"ton\", \"pound\"},\n    \"maximum_weight_recommendation\": {\"milligram\", \"kilogram\", \"microgram\", \"gram\", \"ounce\", \"ton\", \"pound\"},\n    \"voltage\": {\"millivolt\", \"kilovolt\", \"volt\"},\n    \"wattage\": {\"kilowatt\", \"watt\"},\n    \"item_volume\": {\"cubic foot\", \"microlitre\", \"cup\", \"fluid ounce\", \"centilitre\", \"imperial gallon\", \"pint\", \"decilitre\", \"litre\", \"millilitre\", \"quart\", \"cubic inch\", \"gallon\"}\n}\n\nall_units = set.union(*entity_unit_map.values())\nall_units.add(\"unknown\")  # Add \"unknown\" as a possible unit","metadata":{"execution":{"iopub.status.busy":"2024-09-14T08:55:38.718653Z","iopub.execute_input":"2024-09-14T08:55:38.719370Z","iopub.status.idle":"2024-09-14T08:55:38.728463Z","shell.execute_reply.started":"2024-09-14T08:55:38.719305Z","shell.execute_reply":"2024-09-14T08:55:38.727020Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/amazon-ml-cleaned/train_clean.csv\")\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-14T08:55:38.730216Z","iopub.execute_input":"2024-09-14T08:55:38.730686Z","iopub.status.idle":"2024-09-14T08:55:39.440306Z","shell.execute_reply.started":"2024-09-14T08:55:38.730597Z","shell.execute_reply":"2024-09-14T08:55:39.439046Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                          image_link  group_id  entity_name  \\\n0  https://m.media-amazon.com/images/I/61I9XdN6OF...    748919  item_weight   \n1  https://m.media-amazon.com/images/I/71gSRbyXmo...    916768  item_volume   \n2  https://m.media-amazon.com/images/I/61BZ4zrjZX...    459516  item_weight   \n3  https://m.media-amazon.com/images/I/612mrlqiI4...    459516  item_weight   \n4  https://m.media-amazon.com/images/I/617Tl40LOX...    731432  item_weight   \n\n     entity_value  \n0      500.0 gram  \n1         1.0 cup  \n2      0.709 gram  \n3      0.709 gram  \n4  1400 milligram  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_link</th>\n      <th>group_id</th>\n      <th>entity_name</th>\n      <th>entity_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://m.media-amazon.com/images/I/61I9XdN6OF...</td>\n      <td>748919</td>\n      <td>item_weight</td>\n      <td>500.0 gram</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://m.media-amazon.com/images/I/71gSRbyXmo...</td>\n      <td>916768</td>\n      <td>item_volume</td>\n      <td>1.0 cup</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://m.media-amazon.com/images/I/61BZ4zrjZX...</td>\n      <td>459516</td>\n      <td>item_weight</td>\n      <td>0.709 gram</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://m.media-amazon.com/images/I/612mrlqiI4...</td>\n      <td>459516</td>\n      <td>item_weight</td>\n      <td>0.709 gram</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://m.media-amazon.com/images/I/617Tl40LOX...</td>\n      <td>731432</td>\n      <td>item_weight</td>\n      <td>1400 milligram</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"def load_data():\n    train_df = pd.read_csv('/kaggle/input/amazon-ml/train.csv')\n    test_df = pd.read_csv('/kaggle/input/amazon-ml/test.csv')\n    \n    # Create a validation set\n    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n    \n    return train_df, val_df, test_df\n\ntrain_df, val_df, test_df = load_data()\ntrain_df = train_data\n\n# Initialize the LabelEncoder for units\nunit_encoder = LabelEncoder()\nunit_encoder.fit(list(all_units))\n\n# Label encoding for entity_name\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(train_df['entity_name'])","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:17:46.308660Z","iopub.execute_input":"2024-09-14T06:17:46.308990Z","iopub.status.idle":"2024-09-14T06:17:46.924782Z","shell.execute_reply.started":"2024-09-14T06:17:46.308956Z","shell.execute_reply":"2024-09-14T06:17:46.923686Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"LabelEncoder()","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"class ProductImageDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len, is_test=False):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_test = is_test\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        if not is_test:\n            self.df['encoded_entity_name'] = label_encoder.transform(self.df['entity_name'])\n\n    def __len__(self):\n        return len(self.df)\n    \n    def create_placeholder_image():\n        return Image.new('RGB', (224, 224), color='black')  \n\n    def download_image(self, url):\n        try:\n            response = requests.get(url, timeout=10)\n            img = Image.open(BytesIO(response.content)).convert('RGB')\n            return img\n        except Exception as e:\n            print(f\"Error downloading image: {e}\")\n            # Use the placeholder image if there is an error\n            return self.create_placeholder_image()\n\n    def __getitem__(self, idx):\n        image_url = self.df.iloc[idx]['image_link']\n        entity_name = self.df.iloc[idx]['entity_name']\n        \n        img = self.download_image(image_url)\n        \n        img = self.transform(img)\n\n        try:\n            # OCR Extraction using EasyOCR\n            result = reader.readtext(np.array(img.permute(1, 2, 0)), detail=0)\n            text = \" \".join(result)\n        except Exception as e:\n            print(f\"Error in OCR: {e}\")\n            text = \"\"\n\n        # Concatenate entity_name with the OCR extracted text\n        combined_text = f\"{entity_name} {text}\"\n\n        # Tokenize text\n        encoding = self.tokenizer.encode_plus(\n            combined_text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_attention_mask=True,\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        item = {\n            'image': img,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'text': combined_text,\n            'entity_name': entity_name\n        }\n\n        if not self.is_test:\n            item['entity_label'] = torch.tensor(self.df.iloc[idx]['encoded_entity_name'], dtype=torch.long)\n            \n            # Parse entity value and unit\n            entity_value = self.df.iloc[idx]['entity_value']\n            value, unit = self.parse_entity_value(entity_value)\n            item['entity_value'] = torch.tensor(value, dtype=torch.float)\n            \n            # Handle empty unit string\n            if unit == \"\":\n                unit = \"unknown\"\n            item['entity_unit'] = torch.tensor(unit_encoder.transform([unit])[0], dtype=torch.long)\n\n        return item\n\n    def parse_entity_value(self, entity_value):\n        # Handle list-like strings\n        if entity_value.startswith('[') and entity_value.endswith(']'):\n            parts = entity_value[1:-1].split(',')\n        else:\n            parts = entity_value.split()\n        \n        # Extract numeric values\n        values = []\n        for part in parts:\n            try:\n                values.append(float(part.strip()))\n            except ValueError:\n                break\n        \n        # Calculate average if multiple values\n        if values:\n            value = sum(values) / len(values)\n        else:\n            value = 0.0\n        \n        # Extract unit\n        unit = ' '.join(parts[len(values):]).strip()\n        \n#         print(f\"Parsed: value = {value}, unit = {unit}\")  # Debug print\n        return value, unit","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:17:46.926133Z","iopub.execute_input":"2024-09-14T06:17:46.926471Z","iopub.status.idle":"2024-09-14T06:17:46.948812Z","shell.execute_reply.started":"2024-09-14T06:17:46.926435Z","shell.execute_reply":"2024-09-14T06:17:46.947825Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"class EntityExtractionModel(nn.Module):\n    def __init__(self, num_labels, num_units):\n        super(EntityExtractionModel, self).__init__()\n        self.num_labels = num_labels\n        self.num_units = num_units\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)\n        self.unit_classifier = nn.Linear(self.bert.config.hidden_size, num_units)\n\n    def forward(self, input_ids, attention_mask, images):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        value = self.regressor(pooled_output).squeeze(-1)\n        unit_logits = self.unit_classifier(pooled_output)\n        return logits, value, unit_logits","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:17:46.953851Z","iopub.execute_input":"2024-09-14T06:17:46.954157Z","iopub.status.idle":"2024-09-14T06:17:46.964756Z","shell.execute_reply.started":"2024-09-14T06:17:46.954125Z","shell.execute_reply":"2024-09-14T06:17:46.963909Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"# Training & Inference Functions","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, optimizer, classification_criterion, regression_criterion, unit_criterion, epochs):\n    model.to(device)\n    scaler = torch.amp.GradScaler('cuda')\n    best_val_loss = float('inf')\n    \n    for epoch in tqdm(range(epochs), desc='Epoch'):\n        model.train()\n        total_loss = 0\n        for batch in tqdm(train_loader, desc=\"Processing Training Batches\"):\n            optimizer.zero_grad()\n\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            images = batch['image'].to(device)\n            labels = batch['entity_label'].to(device)\n            values = batch['entity_value'].to(device)\n            units = batch['entity_unit'].to(device)\n\n            with autocast():\n                logits, predicted_values, unit_logits = model(input_ids=input_ids, attention_mask=attention_mask, images=images)\n                classification_loss = classification_criterion(logits, labels)\n                regression_loss = regression_criterion(predicted_values, values)\n                unit_loss = unit_criterion(unit_logits, units)\n                loss = classification_loss + regression_loss + unit_loss\n                total_loss += loss.item()\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n        avg_train_loss = total_loss / len(train_loader)\n        \n        # Validation\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=\"Processing Valid Batches\"):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                images = batch['image'].to(device)\n                labels = batch['entity_label'].to(device)\n                values = batch['entity_value'].to(device)\n                units = batch['entity_unit'].to(device)\n\n                logits, predicted_values, unit_logits = model(input_ids=input_ids, attention_mask=attention_mask, images=images)\n                classification_loss = classification_criterion(logits, labels)\n                regression_loss = regression_criterion(predicted_values, values)\n                unit_loss = unit_criterion(unit_logits, units)\n                loss = classification_loss + regression_loss + unit_loss\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(val_loader)\n        \n        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n        \n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save(model.state_dict(), 'best_model.pth')\n            print(\"Saved best model.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:17:46.965903Z","iopub.execute_input":"2024-09-14T06:17:46.966291Z","iopub.status.idle":"2024-09-14T06:17:46.981657Z","shell.execute_reply.started":"2024-09-14T06:17:46.966241Z","shell.execute_reply":"2024-09-14T06:17:46.980846Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def inference(model, test_loader, tokenizer):\n    model.to(device)\n    model.eval()\n    predictions = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            images = batch['image'].to(device)\n            entity_names = batch['entity_name']\n\n            logits, predicted_values, unit_logits = model(input_ids=input_ids, attention_mask=attention_mask, images=images)\n            \n            # Get predicted entity names\n            pred_entity_names = torch.argmax(logits, dim=1)\n            pred_entity_names = label_encoder.inverse_transform(pred_entity_names.cpu().numpy())\n            \n            # Get predicted units\n            pred_units = torch.argmax(unit_logits, dim=1)\n            pred_units = unit_encoder.inverse_transform(pred_units.cpu().numpy())\n            \n            # Format predicted values\n            for idx, (entity_name, value, unit) in enumerate(zip(entity_names, predicted_values.cpu().numpy(), pred_units)):\n                formatted_value = format_value(value, unit, entity_name)\n                predictions.append({\n                    'index': batch['index'][idx] if 'index' in batch else idx,\n                    'prediction': formatted_value\n                })\n\n    return pd.DataFrame(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:17:46.982674Z","iopub.execute_input":"2024-09-14T06:17:46.982956Z","iopub.status.idle":"2024-09-14T06:17:46.997159Z","shell.execute_reply.started":"2024-09-14T06:17:46.982926Z","shell.execute_reply":"2024-09-14T06:17:46.996179Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"entity_unit_map = {\n    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n    'item_weight': {'gram',\n        'kilogram',\n        'microgram',\n        'milligram',\n        'ounce',\n        'pound',\n        'ton'},\n    'maximum_weight_recommendation': {'gram',\n        'kilogram',\n        'microgram',\n        'milligram',\n        'ounce',\n        'pound',\n        'ton'},\n    'voltage': {'kilovolt', 'millivolt', 'volt'},\n    'wattage': {'kilowatt', 'watt'},\n    'item_volume': {'centilitre',\n        'cubic foot',\n        'cubic inch',\n        'cup',\n        'decilitre',\n        'fluid ounce',\n        'gallon',\n        'imperial gallon',\n        'litre',\n        'microlitre',\n        'millilitre',\n        'pint',\n        'quart'}\n}\n\nallowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:17:46.998377Z","iopub.execute_input":"2024-09-14T06:17:46.999042Z","iopub.status.idle":"2024-09-14T06:17:47.012549Z","shell.execute_reply.started":"2024-09-14T06:17:46.998997Z","shell.execute_reply":"2024-09-14T06:17:47.011663Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def common_mistake(unit):\n    if unit in allowed_units:\n        return unit\n    if unit.replace('ter', 'tre') in allowed_units:\n        return unit.replace('ter', 'tre')\n    if unit.replace('feet', 'foot') in allowed_units:\n        return unit.replace('feet', 'foot')\n    return unit","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:17:47.015566Z","iopub.execute_input":"2024-09-14T06:17:47.015841Z","iopub.status.idle":"2024-09-14T06:17:47.026258Z","shell.execute_reply.started":"2024-09-14T06:17:47.015799Z","shell.execute_reply":"2024-09-14T06:17:47.025222Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def parse_string(s):\n    s_stripped = \"\" if s==None or str(s)=='nan' else s.strip()\n    if s_stripped == \"\":\n        return None, None\n    pattern = re.compile(r'^-?\\d+(\\.\\d+)?\\s+[a-zA-Z\\s]+$')\n    if not pattern.match(s_stripped):\n        raise ValueError(\"Invalid format in {}\".format(s))\n    parts = s_stripped.split(maxsplit=1)\n    number = float(parts[0])\n    unit = common_mistake(parts[1])\n    if unit not in allowed_units:\n        raise ValueError(\"Invalid unit [{}] found in {}. Allowed units: {}\".format(\n            unit, s, allowed_units))\n    return number, unit","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:17:47.027881Z","iopub.execute_input":"2024-09-14T06:17:47.028302Z","iopub.status.idle":"2024-09-14T06:17:47.036281Z","shell.execute_reply.started":"2024-09-14T06:17:47.028250Z","shell.execute_reply":"2024-09-14T06:17:47.035395Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"def create_placeholder_image(image_save_path):\n    try:\n        placeholder_image = Image.new('RGB', (100, 100), color='black')\n        placeholder_image.save(image_save_path)\n    except Exception as e:\n        return","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:17:47.037482Z","iopub.execute_input":"2024-09-14T06:17:47.037786Z","iopub.status.idle":"2024-09-14T06:17:47.047653Z","shell.execute_reply.started":"2024-09-14T06:17:47.037754Z","shell.execute_reply":"2024-09-14T06:17:47.046827Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def download_image(image_link, save_folder, retries=3, delay=3):\n    if not isinstance(image_link, str):\n        return\n\n    filename = Path(image_link).name\n    image_save_path = os.path.join(save_folder, filename)\n\n    if os.path.exists(image_save_path):\n        return\n\n    for _ in range(retries):\n        try:\n            urllib.request.urlretrieve(image_link, image_save_path)\n            return\n        except:\n            time.sleep(delay)\n    \n    create_placeholder_image(image_save_path) #Create a black placeholder image for invalid links/images","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:17:47.048765Z","iopub.execute_input":"2024-09-14T06:17:47.049105Z","iopub.status.idle":"2024-09-14T06:17:47.062499Z","shell.execute_reply.started":"2024-09-14T06:17:47.049066Z","shell.execute_reply":"2024-09-14T06:17:47.061686Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"def download_images(image_links, download_folder, allow_multiprocessing=True):\n    if not os.path.exists(download_folder):\n        os.makedirs(download_folder)\n\n    if allow_multiprocessing:\n        download_image_partial = partial(\n            download_image, save_folder=download_folder, retries=3, delay=3)\n\n        with multiprocessing.Pool(64) as pool:\n            list(tqdm(pool.imap(download_image_partial, image_links), total=len(image_links)))\n            pool.close()\n            pool.join()\n    else:\n        for image_link in tqdm(image_links, total=len(image_links)):\n            download_image(image_link, save_folder=download_folder, retries=3, delay=3)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:17:47.063530Z","iopub.execute_input":"2024-09-14T06:17:47.063876Z","iopub.status.idle":"2024-09-14T06:17:47.072935Z","shell.execute_reply.started":"2024-09-14T06:17:47.063834Z","shell.execute_reply":"2024-09-14T06:17:47.072178Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def format_value(value, unit, entity_name):\n    if entity_name not in entity_unit_map or unit not in entity_unit_map[entity_name]:\n        return \"\"\n    return f\"{value:.2f} {unit}\".strip()","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:17:47.073862Z","iopub.execute_input":"2024-09-14T06:17:47.074268Z","iopub.status.idle":"2024-09-14T06:17:47.083899Z","shell.execute_reply.started":"2024-09-14T06:17:47.074227Z","shell.execute_reply":"2024-09-14T06:17:47.083167Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"markdown","source":"*putting it all together*","metadata":{}},{"cell_type":"code","source":"# Step 1: Set device to GPU 2\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:17:47.084964Z","iopub.execute_input":"2024-09-14T06:17:47.085562Z","iopub.status.idle":"2024-09-14T06:17:47.095469Z","shell.execute_reply.started":"2024-09-14T06:17:47.085508Z","shell.execute_reply":"2024-09-14T06:17:47.094618Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"def main():\n    # Initialize tokenizer\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    # Create datasets and dataloaders\n    train_dataset = ProductImageDataset(train_df, tokenizer, MAX_LEN)\n    val_dataset = ProductImageDataset(val_df, tokenizer, MAX_LEN)\n    test_dataset = ProductImageDataset(test_df, tokenizer, MAX_LEN, is_test=True)\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\n    # Initialize model\n    num_labels = len(label_encoder.classes_)\n    num_units = len(set.union(*entity_unit_map.values()))\n    model = EntityExtractionModel(num_labels=num_labels, num_units=num_units).to(device)\n\n    # Initialize optimizer and loss functions\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n    classification_criterion = nn.CrossEntropyLoss()\n    regression_criterion = nn.MSELoss()\n    unit_criterion = nn.CrossEntropyLoss()\n\n    # Train the model\n    train_model(model, train_loader, val_loader, optimizer, classification_criterion, regression_criterion, unit_criterion, EPOCHS)\n\n    # Load best model for inference\n    model.load_state_dict(torch.load('best_model.pth'))\n\n    # Perform inference on test set\n    predictions_df = inference(model, test_loader, tokenizer)\n    predictions_df.to_csv('output.csv', index=False)\n    print(\"Predictions saved to output.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:17:47.096641Z","iopub.execute_input":"2024-09-14T06:17:47.096936Z","iopub.status.idle":"2024-09-14T06:17:47.107440Z","shell.execute_reply.started":"2024-09-14T06:17:47.096903Z","shell.execute_reply":"2024-09-14T06:17:47.106457Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:17:47.108566Z","iopub.execute_input":"2024-09-14T06:17:47.109172Z","iopub.status.idle":"2024-09-14T06:19:50.669844Z","shell.execute_reply.started":"2024-09-14T06:17:47.109108Z","shell.execute_reply":"2024-09-14T06:19:50.668125Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nEpoch:   0%|          | 0/5 [00:00<?, ?it/s]\nProcessing Training Batches:   0%|          | 0/8052 [00:00<?, ?it/s]\u001b[A\nProcessing Training Batches:   0%|          | 1/8052 [00:12<27:24:45, 12.26s/it]\u001b[A\nProcessing Training Batches:   0%|          | 2/8052 [00:24<27:02:04, 12.09s/it]\u001b[A\nProcessing Training Batches:   0%|          | 3/8052 [00:35<26:15:29, 11.74s/it]\u001b[A\nProcessing Training Batches:   0%|          | 4/8052 [00:45<25:05:21, 11.22s/it]\u001b[A\nProcessing Training Batches:   0%|          | 5/8052 [00:57<24:55:29, 11.15s/it]\u001b[A\nProcessing Training Batches:   0%|          | 6/8052 [01:07<24:39:20, 11.03s/it]\u001b[A\nProcessing Training Batches:   0%|          | 7/8052 [01:18<24:35:38, 11.01s/it]\u001b[A\nProcessing Training Batches:   0%|          | 8/8052 [01:29<24:33:33, 10.99s/it]\u001b[A\nProcessing Training Batches:   0%|          | 9/8052 [01:40<24:12:18, 10.83s/it]\u001b[A\nProcessing Training Batches:   0%|          | 10/8052 [02:01<27:10:10, 12.16s/it]\u001b[A\nEpoch:   0%|          | 0/5 [02:01<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[58], line 26\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m unit_criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassification_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregression_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Load best model for inference\u001b[39;00m\n\u001b[1;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n","Cell \u001b[0;32mIn[48], line 8\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, classification_criterion, regression_criterion, unit_criterion, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Training Batches\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[46], line 41\u001b[0m, in \u001b[0;36mProductImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# OCR Extraction using EasyOCR\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(result)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/easyocr/easyocr.py:456\u001b[0m, in \u001b[0;36mReader.readtext\u001b[0;34m(self, image, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, min_size, contrast_ths, adjust_contrast, filter_ths, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, y_ths, x_ths, add_margin, threshold, bbox_min_score, bbox_min_size, max_candidates, output_format)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03mimage: file path or numpy-array or a byte stream object\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    454\u001b[0m img, img_cv_grey \u001b[38;5;241m=\u001b[39m reformat_input(image)\n\u001b[0;32m--> 456\u001b[0m horizontal_list, free_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mmin_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mslope_ths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mslope_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mycenter_ths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mycenter_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mheight_ths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mheight_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth_ths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwidth_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43madd_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43madd_margin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreformat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_candidates\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# get the 1st result from hor & free list as self.detect returns a list of depth 3\u001b[39;00m\n\u001b[1;32m    467\u001b[0m horizontal_list, free_list \u001b[38;5;241m=\u001b[39m horizontal_list[\u001b[38;5;241m0\u001b[39m], free_list[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/easyocr/easyocr.py:321\u001b[0m, in \u001b[0;36mReader.detect\u001b[0;34m(self, img, min_size, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, add_margin, reformat, optimal_num_chars, threshold, bbox_min_score, bbox_min_size, max_candidates)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reformat:\n\u001b[1;32m    319\u001b[0m     img, img_cv_grey \u001b[38;5;241m=\u001b[39m reformat_input(img)\n\u001b[0;32m--> 321\u001b[0m text_box_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_textbox\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m                            \u001b[49m\u001b[43moptimal_num_chars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimal_num_chars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m horizontal_list_agg, free_list_agg \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_box \u001b[38;5;129;01min\u001b[39;00m text_box_list:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/easyocr/detection.py:95\u001b[0m, in \u001b[0;36mget_textbox\u001b[0;34m(detector, image, canvas_size, mag_ratio, text_threshold, link_threshold, low_text, poly, device, optimal_num_chars, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     94\u001b[0m estimate_num_chars \u001b[38;5;241m=\u001b[39m optimal_num_chars \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m bboxes_list, polys_list \u001b[38;5;241m=\u001b[39m \u001b[43mtest_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate_num_chars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimate_num_chars:\n\u001b[1;32m    100\u001b[0m     polys_list \u001b[38;5;241m=\u001b[39m [[p \u001b[38;5;28;01mfor\u001b[39;00m p, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(polys, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mabs\u001b[39m(optimal_num_chars \u001b[38;5;241m-\u001b[39m x[\u001b[38;5;241m1\u001b[39m]))]\n\u001b[1;32m    101\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m polys \u001b[38;5;129;01min\u001b[39;00m polys_list]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/easyocr/detection.py:46\u001b[0m, in \u001b[0;36mtest_net\u001b[0;34m(canvas_size, mag_ratio, net, image, text_threshold, link_threshold, low_text, poly, device, estimate_num_chars)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 46\u001b[0m     y, feature \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m boxes_list, polys_list \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m y:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# make score and link map\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/easyocr/craft.py:60\u001b[0m, in \u001b[0;36mCRAFT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Base network \"\"\"\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     sources \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" U network \"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([sources[\u001b[38;5;241m0\u001b[39m], sources[\u001b[38;5;241m1\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/easyocr/model/modules.py:68\u001b[0m, in \u001b[0;36mvgg16_bn.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 68\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     h_relu2_2 \u001b[38;5;241m=\u001b[39m h\n\u001b[1;32m     70\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice2(h)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import os\nimport random\nimport warnings\nimport requests\nimport multiprocessing\nimport re\nfrom io import BytesIO\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom PIL import Image\nimport easyocr\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torchvision import transforms, models\n\nfrom transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n\nprint(\"Libraries Imported!\")\n\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Hyperparameters\nBATCH_SIZE = 32\nEPOCHS = 10\nLEARNING_RATE = 2e-5\nMAX_LEN = 128\nNUM_WORKERS = multiprocessing.cpu_count()\n\n# Initialize EasyOCR Reader for text extraction\nreader = easyocr.Reader(['en'], gpu=True)\n\nentity_unit_map = {\n    \"width\": {\"centimetre\", \"foot\", \"millimetre\", \"metre\", \"inch\", \"yard\"},\n    \"depth\": {\"centimetre\", \"foot\", \"millimetre\", \"metre\", \"inch\", \"yard\"},\n    \"height\": {\"centimetre\", \"foot\", \"millimetre\", \"metre\", \"inch\", \"yard\"},\n    \"item_weight\": {\"milligram\", \"kilogram\", \"microgram\", \"gram\", \"ounce\", \"ton\", \"pound\"},\n    \"maximum_weight_recommendation\": {\"milligram\", \"kilogram\", \"microgram\", \"gram\", \"ounce\", \"ton\", \"pound\"},\n    \"voltage\": {\"millivolt\", \"kilovolt\", \"volt\"},\n    \"wattage\": {\"kilowatt\", \"watt\"},\n    \"item_volume\": {\"cubic foot\", \"microlitre\", \"cup\", \"fluid ounce\", \"centilitre\", \"imperial gallon\", \"pint\", \"decilitre\", \"litre\", \"millilitre\", \"quart\", \"cubic inch\", \"gallon\"}\n}\n\nall_units = set.union(*entity_unit_map.values())\nall_units.add(\"unknown\")\n\ndef load_data():\n    train_df = pd.read_csv('/kaggle/input/amazon-ml/train.csv')\n    test_df = pd.read_csv('/kaggle/input/amazon-ml/test.csv')\n    \n    train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n    \n    return train_df, val_df, test_df\n\ntrain_df, val_df, test_df = load_data()\ntrain_df = train_data_filtered_2\n\nunit_encoder = LabelEncoder()\nunit_encoder.fit(list(all_units))\n\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(train_df['entity_name'])\n\nclass ProductImageDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len, is_test=False):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_test = is_test\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        if not is_test:\n            self.df['encoded_entity_name'] = label_encoder.transform(self.df['entity_name'])\n\n    def __len__(self):\n        return len(self.df)\n    \n    @staticmethod\n    def create_placeholder_image():\n        return Image.new('RGB', (224, 224), color='black')  \n\n    def download_image(self, url):\n        try:\n            response = requests.get(url, timeout=10)\n            img = Image.open(BytesIO(response.content)).convert('RGB')\n            return img\n        except Exception as e:\n            print(f\"Error downloading image: {e}\")\n            return self.create_placeholder_image()\n\n    def __getitem__(self, idx):\n        image_url = self.df.iloc[idx]['image_link']\n        entity_name = self.df.iloc[idx]['entity_name']\n        \n        img = self.download_image(image_url)\n        img = self.transform(img)\n\n        try:\n            result = reader.readtext(np.array(img.permute(1, 2, 0).cpu()), detail=0)\n            text = \" \".join(result)\n        except Exception as e:\n            print(f\"Error in OCR: {e}\")\n            text = \"\"\n\n        combined_text = f\"{entity_name} {text}\"\n\n        encoding = self.tokenizer.encode_plus(\n            combined_text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_attention_mask=True,\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        item = {\n            'image': img,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'text': combined_text,\n            'entity_name': entity_name\n        }\n\n        if not self.is_test:\n            item['entity_label'] = torch.tensor(self.df.iloc[idx]['encoded_entity_name'], dtype=torch.long)\n            \n            entity_value = self.df.iloc[idx]['entity_value']\n            value, unit = self.parse_entity_value(entity_value)\n            item['entity_value'] = torch.tensor(value, dtype=torch.float)\n            \n            if unit == \"\":\n                unit = \"unknown\"\n            item['entity_unit'] = torch.tensor(unit_encoder.transform([unit])[0], dtype=torch.long)\n\n        return item\n\n    def parse_entity_value(self, entity_value):\n        if entity_value.startswith('[') and entity_value.endswith(']'):\n            parts = entity_value[1:-1].split(',')\n        else:\n            parts = entity_value.split()\n        \n        values = []\n        for part in parts:\n            try:\n                values.append(float(part.strip()))\n            except ValueError:\n                break\n        \n        if values:\n            value = sum(values) / len(values)\n        else:\n            value = 0.0\n        \n        unit = ' '.join(parts[len(values):]).strip()\n        \n        return value, unit\n\nclass EntityExtractionModel(nn.Module):\n    def __init__(self, num_labels, num_units):\n        super(EntityExtractionModel, self).__init__()\n        self.num_labels = num_labels\n        self.num_units = num_units\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.cnn = models.resnet50(pretrained=True)\n        self.cnn.fc = nn.Identity()\n        \n        self.dropout = nn.Dropout(0.3)\n        self.classifier = nn.Linear(self.bert.config.hidden_size + 2048, num_labels)\n        self.regressor = nn.Linear(self.bert.config.hidden_size + 2048, 1)\n        self.unit_classifier = nn.Linear(self.bert.config.hidden_size + 2048, num_units)\n\n    def forward(self, input_ids, attention_mask, images):\n        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        bert_pooled = bert_output.pooler_output\n        \n        cnn_features = self.cnn(images)\n        \n        combined_features = torch.cat((bert_pooled, cnn_features), dim=1)\n        combined_features = self.dropout(combined_features)\n        \n        logits = self.classifier(combined_features)\n        value = self.regressor(combined_features).squeeze(-1)\n        unit_logits = self.unit_classifier(combined_features)\n        \n        return logits, value, unit_logits\n\ndef train_model(model, train_loader, val_loader, optimizer, scheduler, classification_criterion, regression_criterion, unit_criterion, epochs):\n    scaler = GradScaler()\n    best_val_loss = float('inf')\n    \n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            optimizer.zero_grad()\n\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            images = batch['image'].to(device)\n            labels = batch['entity_label'].to(device)\n            values = batch['entity_value'].to(device)\n            units = batch['entity_unit'].to(device)\n\n            with autocast():\n                logits, predicted_values, unit_logits = model(input_ids=input_ids, attention_mask=attention_mask, images=images)\n                classification_loss = classification_criterion(logits, labels)\n                regression_loss = regression_criterion(predicted_values, values)\n                unit_loss = unit_criterion(unit_logits, units)\n                loss = classification_loss + regression_loss + unit_loss\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n\n            total_loss += loss.item()\n\n        avg_train_loss = total_loss / len(train_loader)\n        \n        val_loss = validate_model(model, val_loader, classification_criterion, regression_criterion, unit_criterion)\n        \n        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}')\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), 'best_model.pth')\n            print(\"Saved best model.\")\n\ndef validate_model(model, val_loader, classification_criterion, regression_criterion, unit_criterion):\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            images = batch['image'].to(device)\n            labels = batch['entity_label'].to(device)\n            values = batch['entity_value'].to(device)\n            units = batch['entity_unit'].to(device)\n\n            with autocast():\n                logits, predicted_values, unit_logits = model(input_ids=input_ids, attention_mask=attention_mask, images=images)\n                classification_loss = classification_criterion(logits, labels)\n                regression_loss = regression_criterion(predicted_values, values)\n                unit_loss = unit_criterion(unit_logits, units)\n                loss = classification_loss + regression_loss + unit_loss\n            val_loss += loss.item()\n\n    return val_loss / len(val_loader)\n\ndef inference(model, test_loader, tokenizer):\n    model.eval()\n    predictions = []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Inference\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            images = batch['image'].to(device)\n            entity_names = batch['entity_name']\n\n            with autocast():\n                logits, predicted_values, unit_logits = model(input_ids=input_ids, attention_mask=attention_mask, images=images)\n            \n            pred_entity_names = torch.argmax(logits, dim=1)\n            pred_entity_names = label_encoder.inverse_transform(pred_entity_names.cpu().numpy())\n            \n            pred_units = torch.argmax(unit_logits, dim=1)\n            pred_units = unit_encoder.inverse_transform(pred_units.cpu().numpy())\n            \n            for idx, (entity_name, value, unit) in enumerate(zip(entity_names, predicted_values.cpu().numpy(), pred_units)):\n                formatted_value = format_value(value, unit, entity_name)\n                predictions.append({\n                    'index': idx,\n                    'prediction': formatted_value\n                })\n\n    return pd.DataFrame(predictions)\n\ndef format_value(value, unit, entity_name):\n    if entity_name not in entity_unit_map or unit not in entity_unit_map[entity_name]:\n        return \"\"\n    return f\"{value:.2f} {unit}\".strip()\n\ndef main():\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    train_dataset = ProductImageDataset(train_df, tokenizer, MAX_LEN)\n    val_dataset = ProductImageDataset(val_df, tokenizer, MAX_LEN)\n    test_dataset = ProductImageDataset(test_df, tokenizer, MAX_LEN, is_test=True)\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n\n    num_labels = len(label_encoder.classes_)\n    num_units = len(unit_encoder.classes_)\n    model = EntityExtractionModel(num_labels=num_labels, num_units=num_units).to(device)\n\n    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n    total_steps = len(train_loader) * EPOCHS\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n    \n    classification_criterion = nn.CrossEntropyLoss()\n    regression_criterion = nn.MSELoss()\n    unit_criterion = nn.CrossEntropyLoss()\n\n    train_model(model, train_loader, val_loader, optimizer, scheduler, classification_criterion, regression_criterion, unit_criterion, EPOCHS)\n\n    model.load_state_dict(torch.load('best_model.pth'))\n\n    predictions_df = inference(model, test_loader, tokenizer)\n    predictions_df.to_csv('output.csv', index=False)\n    print(\"Predictions saved to output.csv\")\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:19:50.671085Z","iopub.status.idle":"2024-09-14T06:19:50.671538Z","shell.execute_reply.started":"2024-09-14T06:19:50.671333Z","shell.execute_reply":"2024-09-14T06:19:50.671354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}