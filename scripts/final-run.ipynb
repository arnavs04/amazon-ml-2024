{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9398471,"sourceType":"datasetVersion","datasetId":5693856},{"sourceId":9399301,"sourceType":"datasetVersion","datasetId":5705244},{"sourceId":9403424,"sourceType":"datasetVersion","datasetId":5708645}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport easyocr\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# Initialize EasyOCR reader\nreader = easyocr.Reader(['en'])\n\ndef download_image(image_url):\n    try:\n        response = requests.get(image_url)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n        return image\n    except Exception as e:\n        print(f\"Error downloading image: {e}\")\n        return None\n\ndef extract_text_from_image(image):\n    try:\n        result = reader.readtext(np.array(image))\n        return ' '.join([text for _, text, _ in result])\n    except Exception as e:\n        print(f\"Error extracting text from image: {e}\")\n        return \"\"\n\ndef extract_number(text):\n    numbers = re.findall(r'\\d+\\.?\\d*', text)\n    return float(numbers[0]) if numbers else None\n\ndef process_row(row, most_frequent_units):\n    index = row['index']\n    entity_name = row['entity_name']\n    image_url = row['image_link']\n\n    image = download_image(image_url)\n    if image is not None:\n        extracted_text = extract_text_from_image(image)\n        number = extract_number(extracted_text)\n        \n        if number is not None and entity_name in most_frequent_units:\n            unit = most_frequent_units[entity_name]\n            prediction = f\"{number:.2f} {unit}\"\n        else:\n            prediction = \"\"\n    else:\n        prediction = \"\"\n\n    return {\n        'index': index,\n        'prediction': prediction\n    }\n\ndef main():\n    # Load data\n    train_data = pd.read_csv('/kaggle/input/amazon-ml-cleaned/train_clean.csv')\n    test_data = pd.read_csv('/kaggle/input/amazon-ml/test.csv')\n    test_data = test_data.iloc[65594:][::-1]\n\n    print(f\"Training data size: {len(train_data)}\")\n    print(f\"Test data size: {len(test_data)}\")\n\n    # Analyze most frequent unit for each entity_name\n    entity_unit_freq = defaultdict(lambda: defaultdict(int))\n    for _, row in train_data.iterrows():\n        entity_name = row['entity_name']\n        entity_value = row['entity_value']\n        unit = re.findall(r'[a-zA-Z]+', entity_value)[-1] if re.findall(r'[a-zA-Z]+', entity_value) else ''\n        entity_unit_freq[entity_name][unit] += 1\n\n    most_frequent_units = {entity: max(units, key=units.get) \n                           for entity, units in entity_unit_freq.items()}\n\n    print(\"\\nMost frequent units for each entity:\")\n    for entity, unit in most_frequent_units.items():\n        print(f\"{entity}: {unit}\")\n\n    # Process test data in batches\n    batch_size = 100\n    predictions = []\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        futures = []\n        for i in range(0, len(test_data), batch_size):\n            batch = test_data.iloc[i:i+batch_size]\n            for _, row in batch.iterrows():\n                futures.append(executor.submit(process_row, row, most_frequent_units))\n\n        for future in tqdm(as_completed(futures), total=len(test_data), desc=\"Processing test data\"):\n            predictions.append(future.result())\n\n    # Save predictions\n    output_df = pd.DataFrame(predictions)\n    output_df.to_csv('submission.csv', index=False)\n    print(\"\\nPredictions saved to submission.csv\")\n    # Display the first few rows of the output\n    print(\"\\nFirst few rows of the output:\")\n    print(output_df.head(30).to_string())\n\n    # Verify format\n    valid_format = output_df['prediction'].apply(lambda x: bool(re.match(r'^\\d+\\.\\d{2} [a-zA-Z]+$', x)) if x else True).all()\n    print(f\"\\nOutput format is {'valid' if valid_format else 'invalid'}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-16T04:17:03.785252Z","iopub.execute_input":"2024-09-16T04:17:03.785951Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/easyocr/detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n/opt/conda/lib/python3.10/site-packages/easyocr/recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Training data size: 257652\nTest data size: 65593\n\nMost frequent units for each entity:\nitem_weight: gram\nitem_volume: millilitre\nvoltage: volt\nwattage: watt\nmaximum_weight_recommendation: kilogram\nheight: centimetre\ndepth: centimetre\nwidth: centimetre\nError extracting text from image: CUDA out of memory. Tried to allocate 1.39 GiB. GPU 0 has a total capacity of 15.89 GiB of which 959.12 MiB is free. Process 8158 has 14.95 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 18.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.89 GiB of which 959.12 MiB is free. Process 8158 has 14.95 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 18.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.89 GiB of which 959.12 MiB is free. Process 8158 has 14.95 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 18.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.89 GiB of which 959.12 MiB is free. Process 8158 has 14.95 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 18.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.89 GiB of which 959.12 MiB is free. Process 8158 has 14.95 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 18.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.89 GiB of which 959.12 MiB is free. Process 8158 has 14.95 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 18.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 994.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 959.12 MiB is free. Process 8158 has 14.95 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 18.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.18 GiB. GPU 0 has a total capacity of 15.89 GiB of which 959.12 MiB is free. Process 8158 has 14.95 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 18.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 994.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 959.12 MiB is free. Process 8158 has 14.95 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 18.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.89 GiB of which 959.12 MiB is free. Process 8158 has 14.95 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 18.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.89 GiB of which 959.12 MiB is free. Process 8158 has 14.95 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 411.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.89 GiB of which 959.12 MiB is free. Process 8158 has 14.95 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 411.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.01 GiB is free. Process 8158 has 14.88 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 394.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.17 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.01 GiB is free. Process 8158 has 14.88 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 394.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.01 GiB is free. Process 8158 has 14.88 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 439.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.17 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.06 GiB is free. Process 8158 has 14.83 GiB memory in use. Of the allocated memory 12.71 GiB is allocated by PyTorch, and 1.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.89 GiB of which 165.12 MiB is free. Process 8158 has 15.72 GiB memory in use. Of the allocated memory 12.98 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.89 GiB of which 165.12 MiB is free. Process 8158 has 15.72 GiB memory in use. Of the allocated memory 12.60 GiB is allocated by PyTorch, and 2.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.89 GiB of which 165.12 MiB is free. Process 8158 has 15.72 GiB memory in use. Of the allocated memory 12.78 GiB is allocated by PyTorch, and 2.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 994.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 165.12 MiB is free. Process 8158 has 15.72 GiB memory in use. Of the allocated memory 12.73 GiB is allocated by PyTorch, and 2.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError extracting text from image: CUDA out of memory. Tried to allocate 994.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 165.12 MiB is free. Process 8158 has 15.72 GiB memory in use. Of the allocated memory 12.73 GiB is allocated by PyTorch, and 2.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n","output_type":"stream"}]},{"cell_type":"code","source":"test_data = test_data.iloc[65594:][::-1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport easyocr\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# Initialize EasyOCR reader\nreader = easyocr.Reader(['en'])\n\ndef download_image(image_url):\n    try:\n        response = requests.get(image_url)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n        return image\n    except Exception as e:\n        print(f\"Error downloading image: {e}\")\n        return None\n\ndef extract_text_from_image(image):\n    try:\n        result = reader.readtext(np.array(image))\n        return ' '.join([text for _, text, _ in result])\n    except Exception as e:\n        print(f\"Error extracting text from image: {e}\")\n        return \"\"\n\ndef extract_number(text):\n    numbers = re.findall(r'\\d+\\.?\\d*', text)\n    return float(numbers[0]) if numbers else None\n\ndef process_row(row, most_frequent_units):\n    index = row['index']\n    entity_name = row['entity_name']\n    image_url = row['image_link']\n\n    image = download_image(image_url)\n    if image is not None:\n        extracted_text = extract_text_from_image(image)\n        number = extract_number(extracted_text)\n        \n        if number is not None and entity_name in most_frequent_units:\n            unit = most_frequent_units[entity_name]\n            prediction = f\"{number:.2f} {unit}\"\n        else:\n            prediction = \"\"\n    else:\n        prediction = \"\"\n\n    return {\n        'index': index,\n        'prediction': prediction\n    }\n\ndef main():\n    # Load data\n    train_data = pd.read_csv('/kaggle/input/amazon-ml-cleaned/train_clean.csv')\n    test_data = pd.read_csv('/kaggle/input/amazon-ml/test.csv')\n     # Adjust to start from a specific index\n\n    print(f\"Training data size: {len(train_data)}\")\n    print(f\"Test data size: {len(test_data)}\")\n\n    # Analyze most frequent unit for each entity_name\n    entity_unit_freq = defaultdict(lambda: defaultdict(int))\n    for _, row in train_data.iterrows():\n        entity_name = row['entity_name']\n        entity_value = row['entity_value']\n        unit = re.findall(r'[a-zA-Z]+', entity_value)[-1] if re.findall(r'[a-zA-Z]+', entity_value) else ''\n        entity_unit_freq[entity_name][unit] += 1\n\n    most_frequent_units = {entity: max(units, key=units.get) \n                           for entity, units in entity_unit_freq.items()}\n\n    print(\"\\nMost frequent units for each entity:\")\n    for entity, unit in most_frequent_units.items():\n        print(f\"{entity}: {unit}\")\n\n    # Reverse the test_data so it processes from last to first row\n    test_data = test_data.iloc[::-1]\n\n    # Check if the submission file already exists and load it to get the last processed index\n    output_file = 'submission.csv'\n    if pd.io.common.file_exists(output_file):\n        existing_df = pd.read_csv(output_file)\n        last_processed_index = existing_df['index'].min()  # Get the smallest processed index\n        print(f\"Resuming from index {last_processed_index - 1} (backward)\")\n        test_data = test_data[test_data['index'] < last_processed_index]  # Filter rows above the last processed index\n    else:\n        print(\"No previous submission file found. Starting from the last row.\")\n        last_processed_index = float('inf')\n\n    # Process test data in batches and save after every 1000 predictions\n    batch_size = 100\n    save_every = 1000\n    predictions = []\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        futures = []\n        for i in tqdm(range(0, len(test_data), batch_size)):\n            batch = test_data.iloc[i:i+batch_size]\n            for _, row in batch.iterrows():\n                futures.append(executor.submit(process_row, row, most_frequent_units))\n\n            # Collect predictions in batches\n            for future in as_completed(futures):\n                predictions.append(future.result())\n\n            # Save every 'save_every' rows\n            if len(predictions) >= save_every or i + batch_size >= len(test_data):\n                output_df = pd.DataFrame(predictions)\n                # Append to CSV file\n                output_df.to_csv(output_file, mode='a', header=not pd.io.common.file_exists(output_file), index=False)\n                print(f\"\\nSaved {len(predictions)} rows to {output_file}\")\n\n                # Clear predictions for the next batch\n                predictions = []\n\n    print(\"\\nPredictions saved to submission.csv\")\n    \n    # Verify format of the output (optional)\n    output_df = pd.read_csv(output_file)\n    valid_format = output_df['prediction'].apply(lambda x: bool(re.match(r'^\\d+\\.\\d{2} [a-zA-Z]+$', x)) if x else True).all()\n    print(f\"\\nOutput format is {'valid' if valid_format else 'invalid'}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T04:10:02.275527Z","iopub.status.idle":"2024-09-16T04:10:02.275904Z","shell.execute_reply.started":"2024-09-16T04:10:02.275716Z","shell.execute_reply":"2024-09-16T04:10:02.275735Z"},"trusted":true},"execution_count":null,"outputs":[]}]}