# -*- coding: utf-8 -*-
"""collab amazon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gytIuj2sJcyzwSc9-PwlXAYtXATI81ym
"""

!apt-get install -y tesseract-ocr

pip install torch torchvision transformers pillow requests pytesseract

import torch
from PIL import Image
import requests
from io import BytesIO
from transformers import BlipProcessor, BlipForConditionalGeneration
import pytesseract

# Load the BLIP model and processor
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# URL of the image
image_url = "https://m.media-amazon.com/images/I/61QsBSE7jgL.jpg"  # Replace with your image URL

# Download the image
response = requests.get(image_url)
image = Image.open(BytesIO(response.content))

# Preprocess the image for the model
inputs = processor(images=image, return_tensors="pt")

# Generate the caption for the image
output = model.generate(**inputs)

# Decode and print the caption
caption = processor.decode(output[0], skip_special_tokens=True)
print(f"Generated caption: {caption}")

# URL of the image
image_url = "https://m.media-amazon.com/images/I/61QsBSE7jgL.jpg"  # Replace with your image URL

# Download the image
response = requests.get(image_url)
image = Image.open(BytesIO(response.content))

# Perform OCR on the image
text = pytesseract.image_to_string(image)

# Print the extracted text
print("Extracted text from the image:")
print(text)