{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9398471,"sourceType":"datasetVersion","datasetId":5693856},{"sourceId":9399301,"sourceType":"datasetVersion","datasetId":5705244},{"sourceId":9403424,"sourceType":"datasetVersion","datasetId":5708645}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport re\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport pytesseract\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# Define the entity-unit map for validation purposes\nentity_unit_map = {\n    \"width\": {\"centimetre\", \"foot\", \"millimetre\", \"metre\", \"inch\", \"yard\"},\n    \"depth\": {\"centimetre\", \"foot\", \"millimetre\", \"metre\", \"inch\", \"yard\"},\n    \"height\": {\"centimetre\", \"foot\", \"millimetre\", \"metre\", \"inch\", \"yard\"},\n    \"item_weight\": {\"milligram\", \"kilogram\", \"microgram\", \"gram\", \"ounce\", \"ton\", \"pound\"},\n    \"maximum_weight_recommendation\": {\"milligram\", \"kilogram\", \"microgram\", \"gram\", \"ounce\", \"ton\", \"pound\"},\n    \"voltage\": {\"millivolt\", \"kilovolt\", \"volt\"},\n    \"wattage\": {\"kilowatt\", \"watt\"},\n    \"item_volume\": {\"cubic foot\", \"microlitre\", \"cup\", \"fluid ounce\", \"centilitre\", \"imperial gallon\", \"pint\",\n                    \"decilitre\", \"litre\", \"millilitre\", \"quart\", \"cubic inch\", \"gallon\"}\n}\n\n# Check if GPU is available and set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Preprocess text to extract numbers and units\ndef extract_number(text):\n    \"\"\"Extract the first number found in the text.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    numbers = re.findall(r'\\d+\\.?\\d*', text)\n    return numbers[0] if numbers else \"\"\n\ndef extract_units(text):\n    \"\"\"Extract the unit from the text.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    units = re.findall(r'[a-zA-Z]+', text)\n    return units[-1] if units else \"\"\n\n# Download image from URL\ndef download_image(image_url):\n    \"\"\"Download an image from a URL.\"\"\"\n    try:\n        response = requests.get(image_url)\n        image = Image.open(BytesIO(response.content))\n        return image\n    except Exception as e:\n        print(f\"Error downloading image: {e}\")\n        return None\n\n# Extract text from image using OCR\ndef extract_text_from_image(image):\n    \"\"\"Extract text from an image using OCR.\"\"\"\n    try:\n        text = pytesseract.image_to_string(image)\n        return text.strip()\n    except Exception as e:\n        print(f\"Error extracting text from image: {e}\")\n        return \"\"\n\n# Modify process_image to download image and extract text\ndef process_image(row):\n    \"\"\"Process a single image, extract text.\"\"\"\n    image_url = row['image_link']\n    entity_name = row['entity_name']\n    index = row.name\n    \n    # Download the image\n    image = download_image(image_url)\n    if image:\n        # Extract text from the image\n        extracted_text = extract_text_from_image(image)\n        return {'index': index, 'entity_name': entity_name, 'extracted_text': extracted_text}\n    else:\n        print(f\"Failed to process image URL: {image_url}\")\n        return {'index': index, 'entity_name': entity_name, 'extracted_text': ''}\n\n# Validate the predicted unit\ndef validate_unit(entity_name, unit):\n    \"\"\"Validate if the predicted unit is valid for the entity.\"\"\"\n    valid_units = entity_unit_map.get(entity_name, set())\n    return unit if unit in valid_units else \"\"\n\n# Load pre-trained BERT model for unit prediction\ndef load_bert_model(num_labels):\n    \"\"\"Load a pre-trained BERT model for unit classification.\"\"\"\n    print(\"Loading BERT model and tokenizer...\")\n    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    # Move model to the appropriate device\n    model.to(device)\n\n    return model, tokenizer\n\n# Train the model\ndef train_model(model, dataloader, epochs=10):\n    \"\"\"Train the BERT model.\"\"\"\n    print(\"Training the BERT model...\")\n    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            optimizer.zero_grad()\n            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        avg_loss = total_loss / len(dataloader)\n        print(f\"Epoch {epoch+1} complete. Average Loss: {avg_loss:.4f}\")\n\n# Predict units for extracted text\ndef predict_units(extracted_df, bert_model, tokenizer, id_to_unit):\n    \"\"\"Predict units for the extracted texts.\"\"\"\n    predictions = []\n    for idx, row in tqdm(extracted_df.iterrows(), total=len(extracted_df), desc=\"Predicting units\"):\n        index = row['index']\n        entity_name = row['entity_name']\n        extracted_text = row['extracted_text']\n\n        if extracted_text.strip() == '':\n            predictions.append({'index': index, 'prediction': ''})\n            continue\n\n        # Extract numerical value\n        extracted_num = extract_number(extracted_text)\n\n        # Tokenize and encode\n        encoded_input = tokenizer(extracted_text, return_tensors='pt').to(device)\n        with torch.no_grad():\n            outputs = bert_model(**encoded_input)\n            predicted_unit_id = torch.argmax(outputs.logits, dim=1).item()\n\n            # Map prediction to unit and validate\n            predicted_unit = id_to_unit[predicted_unit_id]\n            validated_unit = validate_unit(entity_name, predicted_unit)\n\n            # Format result for item_value\n            item_value = f\"{extracted_num} {validated_unit}\" if validated_unit else extracted_num\n            if validated_unit:\n                item_value = f\"{extracted_num} {validated_unit}\"\n            else:\n                item_value = \"\"\n\n        predictions.append({'index': index, 'prediction': item_value})\n\n    return pd.DataFrame(predictions)\n\n# Main function\ndef main():\n    # Load and preprocess data\n    train_data = pd.read_csv('/kaggle/input/amazon-ml-cleaned/train_clean.csv')\n    test_data = pd.read_csv('/kaggle/input/amazon-ml/test.csv')\n#     train_data = train_data.iloc[:50000]\n#     test_data = test_data.iloc[:100]\n    \n    # Prepare features and labels\n    train_data['numerical_value'] = train_data['entity_value'].apply(extract_number)\n    train_data['unit'] = train_data['entity_value'].apply(extract_units)\n\n    # Map units to numerical values\n    unit_to_id = {unit: i for i, unit in enumerate(set(train_data['unit']))}\n    id_to_unit = {i: unit for unit, i in unit_to_id.items()}\n\n    # Convert units to numerical IDs\n    train_data['unit_id'] = train_data['unit'].map(unit_to_id)\n\n    # Split data\n    X_train, X_val, y_train, y_val = train_test_split(\n        train_data[['entity_value']],\n        train_data['unit_id'],\n        test_size=0.2,\n        random_state=42\n    )\n\n    # Load BERT model\n    bert_model, tokenizer = load_bert_model(num_labels=len(unit_to_id))\n\n    # Tokenize training data\n    def tokenize_data(texts, tokenizer):\n        return tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n\n    train_encodings = tokenize_data(X_train['entity_value'].tolist(), tokenizer)\n    val_encodings = tokenize_data(X_val['entity_value'].tolist(), tokenizer)\n\n    # Create dataloaders\n    def create_dataloader(encodings, labels, batch_size=32):\n        dataset = torch.utils.data.TensorDataset(\n            encodings['input_ids'], encodings['attention_mask'], torch.tensor(labels.values)\n        )\n        return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    train_loader = create_dataloader(train_encodings, y_train)\n    val_loader = create_dataloader(val_encodings, y_val)\n\n    # Train the model\n    train_model(bert_model, train_loader, epochs=5)  # Train for 5 epochs (increase if necessary)\n\n    # Process test images and convert results to DataFrame\n    extracted_df = pd.DataFrame(test_data.apply(process_image, axis=1).tolist())\n\n    # Predict units\n    output_df = predict_units(extracted_df, bert_model, tokenizer, id_to_unit)\n\n    # Save output\n    output_df.to_csv('test_out.csv', index=False)\n    print(\"Results saved to 'test_out.csv'\")\n    print(output_df)  # Print first few rows of the output CSV\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:30:31.750264Z","iopub.execute_input":"2024-09-15T15:30:31.750622Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading BERT model and tokenizer...\n","output_type":"stream"},{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training the BERT model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 6442/6442 [09:04<00:00, 11.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 complete. Average Loss: 0.0191\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 6442/6442 [09:04<00:00, 11.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 complete. Average Loss: 0.0149\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 6442/6442 [09:04<00:00, 11.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 complete. Average Loss: 0.0055\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 6442/6442 [09:04<00:00, 11.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 complete. Average Loss: 0.0012\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 6442/6442 [09:04<00:00, 11.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 complete. Average Loss: 0.0083\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}